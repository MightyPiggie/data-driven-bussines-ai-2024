{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will look into creating multiple models and fine tuning them to get the best possible results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the following models:\n",
    "1. BaggingClassifier\n",
    "2. RandomForestClassifier\n",
    "3. ExtraTreesClassifier\n",
    "4. VotingClassifier (with TBD models)\n",
    "5. GaussianNB\n",
    "6. KNeighborsClassifier\n",
    "7. MLPClassifier\n",
    "8. LinearTreeClassifier\n",
    "9. LinearForestClassifier\n",
    "10. LinearBoostClassifier\n",
    "\n",
    "The choise of the models are not based on any specific reason, but rather to try out different models and see how they perform. There is although one condition, the model needs to be able to give a probability output, as this will be used in order to give a confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier #https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier #https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier #https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB #https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier #https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier #https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "from lineartree import LinearTreeClassifier, LinearForestClassifier, LinearBoostClassifier #https://github.com/cerlymarco/linear-tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "%run data-cleaning.ipynb\n",
    "%run model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"dataset-prorail-clean-3.csv\")\n",
    "df = clean_data(df)\n",
    "\n",
    "df['duur_prog_fh_seconds'] = df['duur_prog_fh'].dt.total_seconds()\n",
    "num_bins = 10\n",
    "df, bin_edges = create_bins(df, 'duur_prog_fh_seconds', num_bins)\n",
    "label_encoder = LabelEncoder()\n",
    "df['duur_prog_fh_seconds_bins_enc'] = label_encoder.fit_transform(df['duur_prog_fh_seconds_bins'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "BaggingClassifier - 0.3943472409152086 - 11.93635055080362\n",
      "RandomForestClassifier - 0.4003520033129724 - 4.738378568965997\n",
      "ExtraTreesClassifier - 0.39828139558960557 - 8.721678802579728\n",
      "GaussianNB - 0.1045656900300238 - 2.4634349969850162\n",
      "NearestNeighborsClassifier - 0.2072678331090175 - 14.98547783843361\n",
      "MLPClassifier - 0.09990682265244849 - 20.779171113193048\n",
      "LinearTreeClassifier - 0.1273423749870587 - 31.453768963336618\n",
      "LinearForestClassifier - 0.4053214618490527 - 2.6103143891629377\n",
      "LinearBoostClassifier - 0.1291023915519205 - 31.390331536313635\n"
     ]
    }
   ],
   "source": [
    "features = ['stm_geo_mld', 'stm_prioriteit', 'stm_oorz_code', 'stm_contractgeb_gst', 'stm_km_van_mld', 'stm_km_tot_mld', 'stm_techn_mld']\n",
    "target = 'duur_prog_fh_seconds_bins_enc'\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
    "train_models(models, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So just looking at the base models without hyper tuning we can see some models definatlly are performing better than others. Because of this we have decided to continue with the following few to see if we can improve the performance:\n",
    "1. BaggingClassifier\n",
    "2. RandomForestClassifier\n",
    "3. ExtraTreesClassifier\n",
    "4. LinearForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
